{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOWA HOUSE PRICE PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this problem we have two datasets , training set(81 variables and 1460 obesrvations) and testing dataset(80 variables and 1459 observations).\n",
    "##### SalePrice is the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#we will import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score , mean_squared_error\n",
    "import xgboost\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "%pylab inline                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading training and testing data from csv file\n",
    "train = pd.read_csv('attachment_train__2_.csv')\n",
    "test = pd.read_csv('attachment_test.csv')\n",
    "labels = train['SalePrice']  #copying the target variable into labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatinating train and test dataset\n",
    "data1 = pd.concat([train,test],ignore_index=True)\n",
    "data1= data1.drop('SalePrice',1) #dropping the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2919\n",
      "1460\n"
     ]
    }
   ],
   "source": [
    "print(data1.shape[0])\n",
    "print(train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alley           2721\n",
       "BsmtCond          82\n",
       "BsmtExposure      82\n",
       "BsmtFinSF1         1\n",
       "BsmtFinSF2         1\n",
       "BsmtFinType1      79\n",
       "BsmtFinType2      80\n",
       "BsmtFullBath       2\n",
       "BsmtHalfBath       2\n",
       "BsmtQual          81\n",
       "BsmtUnfSF          1\n",
       "Electrical         1\n",
       "Exterior1st        1\n",
       "Exterior2nd        1\n",
       "Fence           2348\n",
       "FireplaceQu     1420\n",
       "Functional         2\n",
       "GarageArea         1\n",
       "GarageCars         1\n",
       "GarageCond       159\n",
       "GarageFinish     159\n",
       "GarageQual       159\n",
       "GarageType       157\n",
       "GarageYrBlt      159\n",
       "KitchenQual        1\n",
       "LotFrontage      486\n",
       "MSZoning           4\n",
       "MasVnrArea        23\n",
       "MasVnrType        24\n",
       "MiscFeature     2814\n",
       "PoolQC          2909\n",
       "SaleType           1\n",
       "TotalBsmtSF        1\n",
       "Utilities          2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtering the variables which contain missing values\n",
    "nulls = pd.isnull(data1).sum()\n",
    "nulls[nulls>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the variables whose missing values are greater than 1000\n",
    "data1 = data1.drop(['PoolQC','MiscFeature','FireplaceQu','Fence','Alley'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering out categorical and non categorical variables\n",
    "categorical=data1.columns[data1.dtypes == 'O']\n",
    "non_categorical = data1.columns[data1.dtypes !='O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BldgType', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
       "       'BsmtQual', 'CentralAir', 'Condition1', 'Condition2', 'Electrical',\n",
       "       'ExterCond', 'ExterQual', 'Exterior1st', 'Exterior2nd', 'Foundation',\n",
       "       'Functional', 'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType',\n",
       "       'Heating', 'HeatingQC', 'HouseStyle', 'KitchenQual', 'LandContour',\n",
       "       'LandSlope', 'LotConfig', 'LotShape', 'MSZoning', 'MasVnrType',\n",
       "       'Neighborhood', 'PavedDrive', 'RoofMatl', 'RoofStyle', 'SaleCondition',\n",
       "       'SaleType', 'Street', 'Utilities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling the missing values by taking median and mode\n",
    "data1['BsmtCond'] = data1['BsmtCond'].fillna(data1['BsmtCond'].mode()[0])\n",
    "data1['BsmtExposure'] = data1['BsmtExposure'].fillna(data1['BsmtExposure'].mode()[0])\n",
    "data1['BsmtFinSF1'] = data1['BsmtFinSF1'].fillna(data1['BsmtFinSF1'].median())\n",
    "data1['BsmtFinSF2'] = data1['BsmtFinSF2'].fillna(data1['BsmtFinSF2'].median())\n",
    "data1['BsmtFinType1'] = data1['BsmtFinType1'].fillna(data1['BsmtFinType1'].mode()[0])\n",
    "data1['BsmtFinType2'] = data1['BsmtFinType2'].fillna(data1['BsmtFinType2'].mode()[0])\n",
    "data1['BsmtFullBath'] = data1['BsmtFullBath'].fillna(data1['BsmtFullBath'].median())\n",
    "data1['BsmtHalfBath'] = data1['BsmtHalfBath'].fillna(data1['BsmtHalfBath'].median())\n",
    "data1['BsmtQual'] = data1['BsmtQual'].fillna(data1['BsmtQual'].mode()[0])\n",
    "data1['BsmtUnfSF'] = data1['BsmtUnfSF'].fillna(data1['BsmtUnfSF'].median())\n",
    "data1['Electrical'] = data1['Electrical'].fillna(data1['Electrical'].mode()[0])\n",
    "data1['Exterior1st'] = data1['Exterior1st'].fillna(data1['Exterior1st'].mode()[0])\n",
    "data1['Exterior2nd'] = data1['Exterior2nd'].fillna(data1['Exterior2nd'].mode()[0])\n",
    "data1['Functional'] = data1['Functional'].fillna(data1['Functional'].mode()[0])\n",
    "data1['GarageArea'] = data1['GarageArea'].fillna(data1['GarageArea'].median())\n",
    "data1['GarageCars'] = data1['GarageCars'].fillna(data1['GarageCars'].median())\n",
    "data1['GarageCond'] = data1['GarageCond'].fillna(data1['GarageCond'].mode()[0])\n",
    "data1['GarageFinish'] = data1['GarageFinish'].fillna(data1['GarageFinish'].mode()[0])\n",
    "data1['GarageQual'] = data1['GarageQual'].fillna(data1['GarageQual'].mode()[0])\n",
    "data1['GarageType'] = data1['GarageType'].fillna(data1['GarageType'].mode()[0])\n",
    "data1['GarageYrBlt'] = data1['GarageYrBlt'].fillna(data1['GarageYrBlt'].median())\n",
    "data1['KitchenQual'] = data1['KitchenQual'].fillna(data1['KitchenQual'].mode()[0])\n",
    "data1['LotFrontage'] = data1['LotFrontage'].fillna(data1['LotFrontage'].median())\n",
    "data1['MSZoning'] = data1['MSZoning'].fillna(data1['MSZoning'].mode()[0])\n",
    "data1['MasVnrArea'] = data1['MasVnrArea'].fillna(data1['MasVnrArea'].median())\n",
    "data1['MasVnrType'] = data1['MasVnrType'].fillna(data1['MasVnrType'].mode()[0])\n",
    "data1['SaleType'] = data1['SaleType'].fillna(data1['SaleType'].mode()[0])\n",
    "data1['TotalBsmtSF'] = data1['TotalBsmtSF'].fillna(data1['TotalBsmtSF'].median())\n",
    "data1['Utilities'] = data1['Utilities'].fillna(data1['Utilities'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** we have only 1460 observations , so there will be no problem replacing the missing values by mode or median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dummy variables of categorical variables\n",
    "data1 = pd.get_dummies(data1,columns=categorical,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 233)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the duplicated columns before proceeding to dimensionality reduction\n",
    "final_data1 = data1.loc[:,~data1.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 233)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing PCA(Principle component analysis) which will help to reduce the no of variables\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(whiten=True) #whiten = True improves the predictive accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(final_data1)\n",
    "variance = pd.DataFrame(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96439327, 0.97538589, 0.98290966, 0.98825137, 0.99307069,\n",
       "       0.99727247, 0.99812826, 0.99865352, 0.99913816, 0.99949109,\n",
       "       0.99970898, 0.99977679, 0.99983461, 0.99988191, 0.99992432,\n",
       "       0.99994851, 0.99996724, 0.99998022, 0.99998995, 0.99999428,\n",
       "       0.99999744, 0.99999964, 0.99999976, 0.99999979, 0.99999981,\n",
       "       0.99999983, 0.99999984, 0.99999986, 0.99999986, 0.99999987,\n",
       "       0.99999987, 0.99999988, 0.99999988, 0.99999989, 0.99999989,\n",
       "       0.9999999 , 0.9999999 , 0.9999999 , 0.99999991, 0.99999991,\n",
       "       0.99999991, 0.99999992, 0.99999992, 0.99999992, 0.99999992,\n",
       "       0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999994,\n",
       "       0.99999994, 0.99999994, 0.99999994, 0.99999994, 0.99999995,\n",
       "       0.99999995, 0.99999995, 0.99999995, 0.99999995, 0.99999995,\n",
       "       0.99999996, 0.99999996, 0.99999996, 0.99999996, 0.99999996,\n",
       "       0.99999996, 0.99999996, 0.99999996, 0.99999996, 0.99999997,\n",
       "       0.99999997, 0.99999997, 0.99999997, 0.99999997, 0.99999997,\n",
       "       0.99999997, 0.99999997, 0.99999997, 0.99999997, 0.99999997,\n",
       "       0.99999998, 0.99999998, 0.99999998, 0.99999998, 0.99999998,\n",
       "       0.99999998, 0.99999998, 0.99999998, 0.99999998, 0.99999998,\n",
       "       0.99999998, 0.99999998, 0.99999998, 0.99999998, 0.99999998,\n",
       "       0.99999998, 0.99999998, 0.99999998, 0.99999999, 0.99999999,\n",
       "       0.99999999, 0.99999999, 0.99999999, 0.99999999, 0.99999999,\n",
       "       0.99999999, 0.99999999, 0.99999999, 0.99999999, 0.99999999,\n",
       "       0.99999999, 0.99999999, 0.99999999, 0.99999999, 0.99999999,\n",
       "       0.99999999, 0.99999999, 0.99999999, 0.99999999, 0.99999999,\n",
       "       0.99999999, 0.99999999, 0.99999999, 0.99999999, 0.99999999,\n",
       "       0.99999999, 0.99999999, 0.99999999, 0.99999999, 0.99999999,\n",
       "       0.99999999, 0.99999999, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the explaning power of each variable , first 21 variables has maximum variance. After 21 variables there is not much change in variance . So we will take the first 21 variables which explains the maximum data. This is called as dimensionality reduction , where we reduce the huge no of features to **n** no of features that explains the maximum relationship between the data. This will increase the accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have taken the first 21 features\n",
    "pca = PCA(n_components=21,whiten=True)\n",
    "pca.fit(final_data1)\n",
    "pca_data = pca.transform(final_data1) #transforming the data with 21 features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the data into train and test\n",
    "train_pca = pca_data[:1460,:]\n",
    "test_pca = pca_data[1460:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and selecting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an instant of the models\n",
    "lr=LinearRegression()\n",
    "xgbr = xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "             objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "             validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the models\n",
    "lr.fit(train_pca,labels)\n",
    "xgbr.fit(train_pca,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the SalePrice using the model that we have created\n",
    "y_predlr = lr.predict(test_pca)\n",
    "y_predxgb = xgbr.predict(test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test my model, i have uploaded my predictions to kaggle competition , where it calculated the rmse score. XGBoost has got better score 0.191312 and my linear regression model has got 0.20.\n",
    "But we can improve the rmse score of my xgboost by randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('sample_submission.csv')\n",
    "ids = sample['Id']\n",
    "y_predxgb = pd.DataFrame(y_predxgb)\n",
    "result1 = pd.concat([sample['Id'],y_predxgb],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.columns = ['Id','SalePrice']\n",
    "result1.to_csv('Submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('sample_submission.csv')\n",
    "ids = sample['Id']\n",
    "y_predlr = pd.DataFrame(y_predlr)\n",
    "result1 = pd.concat([sample['Id'],y_predlr],axis=1)\n",
    "result1.columns = ['Id','SalePrice']\n",
    "result1.to_csv('Submissionlr.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will try to use the Randomized search CV to improve score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators':[100,300,500,900,1100,1500],\n",
    "    'base_score':[0.25,0.5,0.75,1],\n",
    "    'booster':['gbtree','gblinear'],\n",
    "    'learning_rate':[0.01,0.1,0.15,0.2,0.25],\n",
    "    'max_depth':[3,5,6,9,10,11,15],\n",
    "    'min_child_weight':[1,2,3,4,5],\n",
    "    'gamma':[0.0,0.1,0.3,0.5,0.7],\n",
    "    'colsample_bytree':[0.1,0.2,0.3,0.4,0.5],\n",
    "    'sampling_method':['uniform','gradient_based']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr1 = xgboost.XGBRegressor()\n",
    "model = RandomizedSearchCV(estimator=xgbr1,param_distributions=params,n_iter=50,cv=5,n_jobs=4,\n",
    "                           scoring='neg_mean_absolute_error',return_train_score=True,verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed: 13.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None, gamma=None,\n",
       "                                          gpu_id=None, importance_type='gain',\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_con...\n",
       "                                                             0.5],\n",
       "                                        'gamma': [0.0, 0.1, 0.3, 0.5, 0.7],\n",
       "                                        'learning_rate': [0.01, 0.1, 0.15, 0.2,\n",
       "                                                          0.25],\n",
       "                                        'max_depth': [3, 5, 6, 9, 10, 11, 15],\n",
       "                                        'min_child_weight': [1, 2, 3, 4, 5],\n",
       "                                        'n_estimators': [100, 300, 500, 900,\n",
       "                                                         1100, 1500],\n",
       "                                        'sampling_method': ['uniform',\n",
       "                                                            'gradient_based']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=True, scoring='neg_mean_absolute_error',\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the model with 4 fold\n",
    "model.fit(train_pca,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sampling_method': 'uniform',\n",
       " 'n_estimators': 900,\n",
       " 'min_child_weight': 3,\n",
       " 'max_depth': 3,\n",
       " 'learning_rate': 0.1,\n",
       " 'gamma': 0.0,\n",
       " 'colsample_bytree': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'base_score': 0.5}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding out the best parameter\n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr2=model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.5, gamma=0.0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "             min_child_weight=3, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=900, n_jobs=0, num_parallel_tree=1,\n",
       "             objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, sampling_method='uniform', scale_pos_weight=1,\n",
       "             subsample=1, tree_method=None, validate_parameters=False,\n",
       "             verbosity=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model with best estimators\n",
    "xgbr2.fit(train_pca,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predxgbr2 = xgbr2.predict(test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([118975.8 , 236520.6 , 188579.73, ..., 168329.22, 125381.56,\n",
       "       257143.33], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predxgbr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('sample_submission.csv')\n",
    "y_predxgbr2 = pd.DataFrame(y_predxgbr2)\n",
    "result2 = pd.concat([sample['Id'],y_predxgbr2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.columns = ['Id','SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.to_csv('Submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we have got the rmse score of 0.18436 which is an improvement from 0.19132"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my kaggle Profile\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
